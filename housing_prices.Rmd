---
title: 'Data analytics for the Ames Housing Dataset'
output:
  html_document:
    code_folding: hide
    css: style.css
    highlight: tango
    number_sections: true
    theme: yeti
    toc: true
    toc_depth: 2
editor_options:
  chunk_output_type: console
---

```{js, echo = FALSE}
// Add the Kaggle competition banner to the top of the page.
let banner_html = '<img id="banner" src="images/housesbanner.png" alt="Banner">';
header = document.getElementById('header');
header.innerHTML = banner_html + header.innerHTML;
```

```{r setup, message=FALSE, include=FALSE}
knitr::opts_chunk$set(
    echo = TRUE,
    fig.align = 'center',
    comment = NA
)

library(dplyr)
library(DT)
library(ggplot2)
library(latex2exp)
library(purrr)
library(rlang)
library(scales)
library(tidyr)

source('descriptions.R')
source('encode.R')
```

# Summary

The work develops two models for predicting the sale price of houses in the
[Ames Housing Dataset](http://www.amstat.org/publications/jse/v19n3/decock.pdf):

  - A multilinear model that aims to give insight into the housing market
  - A (to-be-determined, probably tree-based) model that aims for predictive
    accuracy

# Exploratory data analysis / feature engineering

Feature engineering was done in an iterative process.

  - Individual predictor variables that could explain variance in the target
    were plotted against the target.
  - Variables were transformed and/or new variables were defined to try to
    enhance linear associations visible in the plots.

## Missing data {.tabset}

### Summary {.unnumbered}

After preprocessing that repairs some inconsistencies in the data, display the
count of missing values for individual variables.

### Discussion {.unnumbered}

From the variable descriptions provided in *data_description.txt*, "NA" is a
valid category for certain columns.

Consistency checks revealed that "NA" sometimes indicates missing data, even
in variables where it represents a valid category.

  - Example:  The description of variable BsmtExposure has "NA" indicating "No
    Basement".  But the house with Id 949 has "NA" for BsmtExposure in spite
    of the fact that BsmtQual is "Good", BsmtCond is "Typical", and
    TotalBsmtSF ("Total square feet of basement area") is 936.  For this house,
    the "NA" for BsmtExposure likely indicates a missing value rather than "No
    Basement."

The script *preprocess.R* attempts to repair inconsistencies and replace "NA"
by a string such as "NB" for "No Basement" in cases where it represented a
valid category.

  - In the preprocessed data, only the variable LotFrontage ("Linear feet of
    street connected to property") has a significant number of missing values.

### Missing value counts {.tab-display .unnumbered}

<div class="code-block">

```{r, missing-data, cache=FALSE}
read_data <- function(file) {
    data <- read.csv(file, stringsAsFactors = FALSE)
    return(data)
}
train_data <- read_data('data/train.csv')
test_data <- read_data('data/test.csv')
get_na_counts <- function(data, dataset_name) {
    na_counts <- data.frame(name = colnames(data),
                            count = colSums(is.na(data)),
                            dataset = dataset_name,
                            stringsAsFactors = FALSE) %>%
        arrange(desc(count)) %>%
        filter(count != 0)
    return(na_counts)
}

na_counts_train <- get_na_counts(train_data, 'train')
na_counts_test <- get_na_counts(test_data, 'test')
combined_na_counts <- rbind(na_counts_test, na_counts_train,
                            make.row.names = FALSE) %>%
    arrange(desc(count), desc(dataset), name) %>%
    rename(`missing value count` = count)

# The same (complicated) set of options can be used with DT::datatable
# throughout this file.  A function wrapper is defined for this.
get_datatable <- function(data, page_length = 20) {
    table_alignment <- list(className = 'dt-center', targets = '_all')
    table_with_options <- datatable(
        data,
        rownames = FALSE,
        options = list(
            columnDefs = list(table_alignment),
            pageLength = page_length
         )
    )
    return(table_with_options)
}
get_datatable(combined_na_counts)
```

</div>

## Variable distributions {.tabset}

### Summary {.unnumbered}

Apply transformations and plot the distribution of each variable.

### Discussion {.unnumbered}

The distribution of each variable is plotted.

  - Numeric predictors with only a few values are labeled as discrete, while
    those with many values are labeled as continuous.
  - For variables that were transformed, the distributions before and after the
    transformation are both shown, along with the formula for the
    transformation.
  - The variable descriptions from *data_description.txt* are included with the
    plots.

Transformation of the target variable and the predictor variable GrLivArea
decreased skew.

Several variables have a distribution concentrated into a single value or a
small subset of the values.

  - For example, most houses have zero pool area.

### Target {.tab-display .unnumbered}

<div class="code-block">

<!-- R chunk to keep echoed code in one continuous block -->

```{r, show-plot-target, ref.label='plot-target', eval=FALSE}
```

```{r, plot-target, cache=TRUE, echo=FALSE}
to_plot <- select(train_data, SalePrice) %>%
    mutate(SalePrice = SalePrice / 1000)
fig <- ggplot(to_plot,
              aes(x = SalePrice)) +
    geom_histogram(fill = 'navyblue', bins = 30) +
    scale_x_continuous(name = 'SalePrice',
                       labels = label_dollar(suffix = 'k')) +
    ggtitle('SalePrice')
print(fig)

to_plot <- mutate(to_plot, SalePrice = log(SalePrice))
fig <- ggplot(select(to_plot, SalePrice),
              aes(x = SalePrice)) +
    geom_histogram(fill = 'navyblue', bins = 30) +
    scale_x_continuous(name = 'log(SalePrice)') +
    ggtitle('log(SalePrice)')
print(fig)
cat('transformation:  ', 'price -> log(price)', '\n\n')
```

</div>

### Discrete predictors {.tab-display .unnumbered}

<div class="code-block">

```{r, setup-distributions, cache=FALSE, echo=FALSE}
# Given a vector of variable names, reorder the names to match the order used in
# the original data set (i.e., in csv files and data_description.txt).
all_column_names <- colnames(train_data)
reorder_names <- function(variable_names) {
    reorder_index <- order(match(variable_names, all_column_names))
    variable_names <- variable_names[reorder_index]
}

# For numeric variables with only a few values (such as YrSold), it is not
# initially clear whether the variable should be treated as categorical or
# numeric in, say, a linear model.  For plots, however, it is natural to treat
# these variables as categorical initially.
#
# Define three classes of variables:  categorical, continuous, and discrete
# numeric.  Here 'continuous' is used for numeric variables containing many
# values.

# Note that MSSubClass is a nominal variable labeled with nonconsecutive integer
# values, and it should be treated as categorical.
train_data <- mutate(train_data, MSSubClass = as.factor(MSSubClass))
categorical_predictors <- reorder_names(colnames(
    select(train_data, where(is.character), MSSubClass)
))

discrete_numeric <- reorder_names(c(
    'OverallQual', 'OverallCond', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath',
    'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces',
    'GarageCars', 'MoSold', 'YrSold'
))

continuous_predictors <- reorder_names(setdiff(
    all_column_names,
    c(categorical_predictors, discrete_numeric, 'Id', 'SalePrice')
))

# Helper function that finds the total number of characters needed for the
# labels on the horizontal axis.  If this number is too large, the labels are
# rotated.
nchar_labels <- function(vector_to_plot) {
    # Drop missing values.
    vector_to_plot <- vector_to_plot[!is.na(vector_to_plot)]
    char_vector <- as.character(vector_to_plot)
    label_lengths <- nchar(unique(char_vector))
    return(sum(label_lengths))
}

# Helper function that rotates the labels of the horizontal axis if necessary.
set_label_angle <- function(fig, vector_to_plot) {
    if (nchar_labels(vector_to_plot) > 60) {
        fig <- fig + theme(
            axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)
        )
    }
    return(fig)
}

# Helper function used in setting x-axis labels.
get_discrete_values <- function(vector_to_plot) {
    axis_labels <- seq(min(vector_to_plot),
                       max(vector_to_plot))
    return(axis_labels)
}

```

<!--
In this document, R chunks with labels of the form show-... use the ref.label
option to display the code for two chunks together as one block of code.

During development it is convenient to set cache=TRUE for the chunks that
generate many plots, while keeping cache=FALSE for chunks that have side
effects needed later in the document.  However, in the html output with folded
code, it is distracting to have the echoed code in two blocks, each with its
own 'Hide' or 'Show' button.  Using ref.label in a chunk with eval=FALSE
allows the code from two chunks to be displayed as one block of code.
-->

```{r, show-setup, ref.label=c('setup-distributions', 'discrete-distributions'), eval=FALSE}
```

```{r, discrete-distributions, cache=TRUE, echo=FALSE}
to_plot_categorical <- reorder_names(
    c(categorical_predictors, discrete_numeric)
)
for (variable_name in to_plot_categorical) {
    # The list variable_descriptions is defined in 'descriptions.R'
    description <- variable_descriptions[[variable_name]]
    to_plot <- select(train_data, all_of(variable_name)) %>%
        drop_na()

    # In cases where the variable is an integer, converting it to a factor
    # before plotting decreases the amount of tweaking that is needed for the
    # plot.  However, there may be missing values in the horizontal axis.  In
    # the training set, for instance, there are no samples that have 7
    # bedrooms above ground, so 6 ends up next to 8 without special handling.
    # In order to avoid this, use the limits argument of scale_x_discrete.
    is_int <- is.integer(to_plot[[variable_name]])
    if (is_int) {
        x_axis_labels <- get_discrete_values(to_plot[[variable_name]]) %>%
            as.character
        to_plot[[variable_name]] <- as.factor(to_plot[[variable_name]])
    }

    fig <- ggplot(to_plot, aes_string(x = variable_name)) +
        geom_bar(fill = 'navyblue') +
        ggtitle(variable_name)
    if (is_int) {
        fig <- fig + scale_x_discrete(limits = x_axis_labels)
    } else {
        fig <- set_label_angle(fig, to_plot[[variable_name]])
    }
    print(fig)
    cat(description)
}
```

</div>

### Continuous predictors {.tab-display .unnumbered}

<div class="code-block">

```{r transform, cache=FALSE, echo=FALSE}
# From the consistency checks done by preprocess.R, there are no missing
# values for GarageYrBlt in eiether the training data or the test data.  The
# preprocessing script set this variable to zero for houses without a garage.
# For convenience in dropping data for plotting, temporarily replace zero by
# NA.
bool_index <- train_data$GarageYrBlt == 0
train_data[bool_index, 'GarageYrBlt'] <- NA

# Function used for transforming YearBuilt and GarageYrBlt
transform_year <- function(x) {
    return((x - 1900)^3 / 1e6)
}
transform_year_formula <- 'year -> (year - 1900)^3 / 1e6'

# The information associated with transforming variables is entered as a list
# of lists.  In order to avoid awkward syntax in accessing elements of nested
# list, however, the nested list is used to generated simple 1D lists and
# vectors.  (The reason for using the nested list initially is to group
# together the information for a particular variable.)
transformations <- list(
    GrLivArea = list(func = log,
                     label = 'log(GrLivArea)',
                     formula = 'area -> log(area)'),
    YearBuilt = list(func = transform_year,
                     label = 'transformed YearBuilt',
                     formula = transform_year_formula),
    GarageYrBlt = list(func = transform_year,
                       label = 'transformed GarageYrBlt',
                       formula = transform_year_formula)
)
transformed_variables <- names(transformations)
transformed_variable_labels <- map_chr(transformations, ~ (.)$label)
transformation_functions <- map(transformations, ~ (.)$func)
transformation_formulas <- map_chr(transformations, ~ (.)$formula)

get_variable_label <- function(variable_name) {
    if (variable_name %in% transformed_variables) {
        variable_label <- transformed_variable_labels[variable_name]
    } else {
        variable_label <- variable_name
    }
    return(variable_label)
}

```

```{r, show-transform, ref.label=c('transform', 'continuous-distributions'), eval=FALSE}
```


```{r, continuous-distributions, cache=TRUE, echo=FALSE}
for (variable_name in continuous_predictors) {
    description <- variable_descriptions[[variable_name]]
    to_plot <- select(train_data, all_of(variable_name)) %>%
        drop_na()

    fig <- ggplot(to_plot, aes_string(x = variable_name)) +
        geom_histogram(fill = 'navyblue', bins = 30) +
        ggtitle(variable_name)
    print(fig)

    if (variable_name %in% transformed_variables) {
        # Apply the transformation to the variable.
        func <- transformation_functions[[variable_name]]
        formula <- transformation_formulas[variable_name]
        to_plot[[variable_name]] <- func(
            to_plot[[variable_name]]
        )

        variable_label <- get_variable_label(variable_name)
        fig <- ggplot(to_plot,
                      aes_string(x = variable_name)) +
            geom_histogram(fill = 'navyblue', bins = 30) +
            scale_x_continuous(name = variable_label) +
            ggtitle(variable_label)
        print(fig)

        cat('transformation:  ', formula, '\n\n')
    }

    cat(description)
}
```

</div>

## Which predictors can explain the variance in the sale price? {.tabset}

### Summary {.unnumbered}

Show the fraction of variance in the sale price explained by individual
predictors.

### Discussion {.unnumbered}

ANOVA ($\eta^2$) is used for nominal predictors, and $R^2$ is used for numeric
predictors.

For many of the predictors that individually explain the largest fraction of
variance, the value of $\eta^2$ or $R^2$ was increased by the transformations.

  - Example:  the highest value of $R^2$ (OverallQual) increased from 0.63 to
    0.67 due to the transformation of SalePrice.

### Nominal predictors {.tab-display .unnumbered}

<div class="code-block">

```{r, eta-squared, cache=FALSE}
# Apply transformations to variables in the training data.  The R chunks that
# generate large plots are cached, and so transformations in those chunks
# were done to temporary plotting data frames.
train_data <- mutate(train_data, SalePrice = log(SalePrice))
for (variable_name in transformed_variables) {
    # Apply the transformation to the variable.
    func <- transformation_functions[[variable_name]]
    train_data[[variable_name]] <- func(train_data[[variable_name]])
}

get_linear_model <- function(variable_name,
                             dummy_filter = '',
                             outliers = integer()) {
    # For categorical variables, perform ANOVA and return eta squared along
    # with predictions based on the model.  For numeric variables, do a simple
    # linear regression and return R squared along with predictions.
    #
    # Note that the command aov used for ANOVA in R is a wrapper to lm that
    # provides a summary in the traditional ANOVA form.  This summary doesn't
    # include eta squared.  However, the value is available as r.squared from
    # the summary for lm.  So if aov is used to generate the model, then eta
    # squared is available as summary.lm(model)$r.squared.
    #
    # In the current function, use the lm command both for categorical and
    # numeric predictors.
    data <- filter(train_data, !(Id %in% outliers))
    model_formula <- paste0('SalePrice ~ ', variable_name)
    if  (nchar(dummy_filter) > 0) {
        model_formula <- paste(model_formula, '+', dummy_filter)
        vars <- c(variable_name, dummy_filter)
        data <- select(data, SalePrice, all_of(vars)) %>%
            drop_na()
    } else {
        data <- select(data, SalePrice, all_of(variable_name)) %>%
            drop_na()
    }
    model_formula <- as.formula(model_formula)
    model <-  lm(model_formula, data)
    model$formula <- model_formula
    model$dummy_filter <- dummy_filter
    model$outliers <- outliers
    return(model)
}

anova_models <- map(categorical_predictors, get_linear_model)
names(anova_models) <- categorical_predictors
eta_squared_vec <- map_dbl(anova_models, ~ summary(.)$r.squared)
eta_squared_df <- data.frame(name = categorical_predictors,
                             eta.squared = eta_squared_vec,
                             stringsAsFactors = FALSE) %>%
    arrange(desc(eta.squared)) %>%
    rename(`eta squared` = eta.squared)
get_datatable(eta_squared_df) %>%
    formatRound(columns = 'eta squared', digits = 3)
```

</div>

### Numeric predictors  {.tab-display .unnumbered}

<div class="code-block">

```{r, r-squared, cache=FALSE}
r_squared_variables <- reorder_names(
    c(continuous_predictors, discrete_numeric)
)

linear_models <- map(r_squared_variables, get_linear_model)
names(linear_models) <- r_squared_variables
r_squared_vec <- map_dbl(linear_models, ~ summary(.)$r.squared)
r_squared_df <- data.frame(name = r_squared_variables,
                           r.squared = r_squared_vec,
                           stringsAsFactors = FALSE) %>%
    arrange(desc(r.squared)) %>%
    rename(`R squared` = r.squared)
get_datatable(r_squared_df) %>%
    formatRound(columns = 'R squared', digits = 3)
```

</div>

## Plots of sale price vs individual predictors {#association-plots .tabset}

### Summary {.unnumbered}

Visualize the dependence of sale price on individual predictors.

### Discussion {.unnumbered}

Predictors with large $\eta^2$ or $R^2$ are plotted against sale price.

  - Here "large" is defined to mean $\gtrsim 0.1$.
  - For both $\eta^2$ and $R^2$, there is a gap in the values at around 0.1, and
    predictors falling above the gap are plotted.
  - Predictor distributions and descriptions are also shown.

The linear profile of several of the predictors improved dramatically due to
the transformations.

  - For example, the transformation caused the median values shown in many of
    the box plots to fall roughly on a line.


### Box plots {.tab-display .unnumbered}

<div class="code-block">

```{r, large-eta, cache=FALSE, echo=FALSE}
large_eta_squared <- filter(eta_squared_df, `eta squared` >= 0.085)

generate_box_plot <- function(variable_name, eta_squared,
                              show_encoding = FALSE) {

    description <- variable_descriptions[[variable_name]]
    if (show_encoding) {
        encoded_variable_name <- paste0(variable_name, 'Num')
        vars <- c(variable_name, encoded_variable_name)
        to_plot <- select(train_data, SalePrice, all_of(vars)) %>%
            rename(encoding = .data[[encoded_variable_name]]) %>%
            drop_na()
    } else {
        to_plot <- select(train_data, SalePrice, all_of(variable_name)) %>%
            drop_na()
    }
    to_plot[[variable_name]] <- as.factor(to_plot[[variable_name]]) %>%
        reorder(to_plot$SalePrice, median)

    plot_title <- paste0(variable_name, ', ', '$\\eta^2 = ', eta_squared, '$')
    fig <- ggplot(to_plot, aes_string(x = variable_name)) +
        geom_boxplot(aes_string(y = 'SalePrice', fill = variable_name))
    if (show_encoding) {
        fig <- fig +
            geom_point(aes(y = encoding, color = 'encodings')) +
            scale_color_manual(name = variable_name,
                               values = c('encodings' = 'red')) +
            guides(fill = FALSE)
    }
    fig <- fig + scale_y_continuous(name = 'log(SalePrice)') +
        scale_x_discrete(name = variable_name) +
        ggtitle(TeX(plot_title))
    fig <- set_label_angle(fig, to_plot[[variable_name]])
    print(fig)

    # Bar chart of the distribution
    plot_title <- paste(variable_name, 'distribution')
    fig <- ggplot(to_plot, aes_string(x = variable_name)) +
        geom_bar(fill = 'navyblue') +
        scale_x_discrete(name = variable_name) +
        ggtitle(plot_title)
    fig <- set_label_angle(fig, to_plot[[variable_name]])
    print(fig)

    cat(description)
}
```

```{r, show-large-eta, ref.label=c('large-eta', 'box-plots'), eval=FALSE}
```

```{r, box-plots, cache=TRUE, echo=FALSE}
for (variable_name in large_eta_squared$name) {
    eta_squared <- round(eta_squared_vec[variable_name], digits = 3)
    generate_box_plot(variable_name, eta_squared)
}
```

</div>

### Scatter plots {.tab-display .unnumbered}

<div class="code-block">

```{r, large-r, cache=FALSE, echo=FALSE}
large_r_squared <- filter(r_squared_df, `R squared` >= 0.095)

# Reusable plotting function
plot_simple_fit <- function(variable_name, linear_model, r_squared = 0) {
    outliers <- linear_model$outliers
    dummy_filter <- linear_model$dummy_filter
    description <- variable_descriptions[[variable_name]]
    to_plot <- filter(train_data, !(Id %in% outliers))

    if (nchar(dummy_filter) > 0) {
        to_plot <- filter(to_plot, .data[[dummy_filter]] == 0) %>%
            select(SalePrice, all_of(c(variable_name, dummy_filter))) %>%
            drop_na()
    } else {
        to_plot <-  select(to_plot, SalePrice, all_of(variable_name)) %>%
            drop_na()
    }

    prediction <- predict(linear_model, to_plot)

    to_plot <- mutate(to_plot, PredictedPrice = prediction)

    # Scatter plot
    variable_label <- get_variable_label(variable_name)
    if (as.logical(r_squared)) {
        plot_title <- paste0(
            variable_label, ', ', '$R^2 = ', r_squared, '$'
        )
    } else {
        plot_title <- variable_label
    }
    fig <- ggplot(to_plot, aes_string(x = variable_name)) +
        geom_point(aes(y = SalePrice, color = 'data')) +
        geom_line(aes(y = PredictedPrice, color = 'fit')) +
        scale_color_manual(
            name = NULL,
            values = c(
                'data' = 'navyblue',
                'fit' = 'red'
            )) +
        scale_y_continuous(name = 'log(SalePrice)') +
        ggtitle(TeX(plot_title))

    is_discrete <- variable_name %in% discrete_numeric
    if (is_discrete) {
        breaks <- get_discrete_values(to_plot[[variable_name]])
        labels <- as.character(breaks)
        fig <- fig + scale_x_continuous(
            name = variable_label, breaks = breaks, labels = labels
        )
    } else {
        fig <- fig + scale_x_continuous(name = variable_name)
    }
    print(fig)

    # Histogram or bar chart of the distribution
    plot_title <- paste(variable_label, 'distribution')
    if (is_discrete) {
        to_plot[[variable_name]] <- as.factor(to_plot[[variable_name]])
        fig <- ggplot(to_plot, aes_string(x = variable_name)) +
            geom_bar(fill = 'navyblue') +
            scale_x_discrete(limits = labels)
    } else {
        fig <- ggplot(to_plot, aes_string(x = variable_name)) +
            geom_histogram(fill = 'navyblue', bins = 30) +
            scale_x_continuous(name = variable_label)
    }
    print(fig + ggtitle(plot_title))

    if (variable_name %in% transformed_variables) {
        cat('transformation:  ',
            transformation_formulas[variable_name],
            '\n\n'
        )
    }
    cat(description)
}

```

```{r, show-large-r, ref.label=c('large-r', 'scatter-plots'), eval=FALSE}
```

```{r, scatter-plots, cache=TRUE, echo=FALSE}
for (variable_name in large_r_squared$name) {
    r_squared <- round(r_squared_vec[variable_name], digits = 3)
    linear_model <- linear_models[[variable_name]]
    plot_simple_fit(variable_name, linear_model, r_squared)
}
```

</div>

## Feature engineering

The goal is to define simple features that have a strong linear association
with the target.

### Define dummy variables, exclude outliers {.tabset}

#### Summary {.unnumbered}

Enhance the linear dependence of sale price on individual numeric predictors.

#### Discussion {.unnumbered}

The starting point for the process is the set of predictors that have large
$R^2$.

For certain scatter plots, the fit appears distorted by outliers and/or by
the points at the vertical intercept.

Distortions due to the vertical intercept can be eliminated by defining a
dummy variable.

  - Example:  A dummy variable NoBasement that identifies houses without a
    basement was defined in order to avoid distortions in the fit for
    variables that characterize basement area.

With outliers excluded and appropriate dummy variables defined, $R^2$
increased and the fit shown in scatter plots improved.


#### Changes in $R^2$ {.tab-display .unnumbered}

<div class="code-block">

```{r, dummy-variables, cache=FALSE, echo=FALSE}
# Add dummy variables to data.  From the consistency checks done by the
# preprocessing script, the variables used in defining NoBasement,
# NoGarage, NoFireplace had no missing values in the original data sets
# (both training and test).  However, the current script set houses with no
# garage to have GarageYr to NA to facilitate plotting, and so is.na is used
# to define the variable NoGarage.
define_dummies <- function(data) {
    data <- mutate(data,
                   NoBasement = as.integer(TotalBsmtSF == 0),
                   NoGarage = as.integer(is.na(GarageYrBlt)),
                   NoFireplace = as.integer(Fireplaces == 0),
                   NoSecondFloor = as.integer(SecondFlrSF == 0))
    return(data)
}
train_data <- define_dummies(train_data)

# Now that NoGarage can be used to distinguish houses without a garage,
# GarageYrBlt can be set to an arbitrary value for houses without a garage.
# Choose -0.1 as an arbitrary value to separate these houses in the plot of
# GarageYrBlt.
bool_index <- is.na(train_data$GarageYrBlt)
train_data[bool_index, 'GarageYrBlt'] <- -0.1

basement_related <- c(
    'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1',
    'BsmtFinType2', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath',
    'BsmtHalfBath'
)
garage_related <- c(
    'GarageFinish', 'GarageQual', 'GarageCond', 'GarageType', 'GarageYrBlt',
    'GarageCars', 'GarageArea'
)
fireplace_related <- c('FireplaceQu', 'Fireplaces')
second_floor_related <- 'SecondFlrSF'

# Find the dummy variable that imposes a constraint on the values for a
# variable.
get_dummy_filter <- function(variable_name) {
    dummy <- case_when(
        variable_name %in% basement_related ~ 'NoBasement',
        variable_name %in% garage_related ~ 'NoGarage',
        variable_name %in% fireplace_related ~ 'NoFireplace',
        variable_name %in% second_floor_related ~ 'NoSecondFloor',
        TRUE ~ ''
    )
    return(dummy)
}

bool_index <- !(all_column_names %in% c('Id', 'SalePrice'))
original_predictors <- all_column_names[bool_index]
dummy_filters <- get_dummy_filter(original_predictors)
names(dummy_filters) <- original_predictors

```

```{r, outliers, cache=FALSE, echo=FALSE}
# For certain variables, the fit is visibly off because of a few outliers for
# which the variable has a very large value.  Define a vector of formula
# strings to use for dropping outliers in evaluating the fit of individual
# variables.
outlier_formulas <- c(
    GrLivArea = '>= log(4500)',
    GarageArea = '>= 1240',
    TotalBsmtSF = '>= 3000',
    BsmtFinSF1 = '>= 3000',
    SecondFlrSF = '> 3000',
    LotFrontage = '> 300'
)

# Convert these formulas into a list of outliers.
get_outliers <- function(data, variable_name) {
    if (variable_name %in% names(outlier_formulas)) {
        formula <- parse_expr(paste(
            variable_name, outlier_formulas[[variable_name]]
        ))
        filtered_data <- filter(data, !!formula)
        outliers <- filtered_data$Id
    }
    else {
        outliers <- integer()
    }
    return(outliers)
}

variable_outliers <- map(original_predictors,
                         ~ get_outliers(train_data, .))
names(variable_outliers) <- original_predictors
```

```{r, r-squared-changes, ref.label=c('dummy-variables', 'outliers', 'update-r-squared'), eval=FALSE}
```

```{r, update-r-squared, cache=FALSE, echo=FALSE}
# In order to check how linear fits are improved by the exclusion of outliers
# and/or the addition of dummy variables, generate new linear models.  For
# example, include NoBasement as a second predictor in the finding the
# linear fit for the predictor TotalBsmftSF, and exclude
# variable_outliers[['TotalBsmtSF']] from the fit.
#
# As new fits are generated, append values to vectors that will form a data
# frame showing changes in R squared.
original_model <- character()
updated_model <- character()
original_r_squared <- numeric()
updated_r_squared <- numeric()
for (variable_name in large_r_squared$name) {
    outliers <- variable_outliers[[variable_name]]
    dummy_filter <- dummy_filters[variable_name]
    model_update_needed <- length(outliers) > 0 | nchar(dummy_filter) > 0
    if (model_update_needed) {
        # The elements of dummy_filters are named.  For instance, in the case
        # where variable_name is 'TotalBsmtSF', the variable
        #
        # dummy_filter <- dummy_filters[variable_name]
        #
        # is the following:
        #
        # > dummy_filter
        #  TotalBsmtSF
        #  "NoBasement"
        #
        # If dummy_filter is used in a dplyr select statement, the column
        # 'NoBasement' gets selected and then renamed to TotalBsmtSF.  In
        # order to avoid this, set the name of dummy_filter to NULL.
        names(dummy_filter) <- NULL
        linear_model <- get_linear_model(
            variable_name, dummy_filter, outliers = outliers
        )
        linear_models[[variable_name]] <- linear_model

        original_model <- c(original_model, variable_name)
        if (nchar(dummy_filter) > 0) {
            updated_model <- c(updated_model,
                               paste(variable_name, '+', dummy_filter))
        } else {
            updated_model <- c(updated_model, variable_name)
        }
        original_r_squared <- c(original_r_squared,
                                r_squared_vec[variable_name])
        updated_r_squared <- c(updated_r_squared,
                               summary(linear_model)$r.squared)
    }
}

r_squared_changes <- data.frame(`original model` = original_model,
                                `original R squared` = original_r_squared,
                                `updated model` = updated_model,
                                `updated R squared` = updated_r_squared,
                                check.names = FALSE,
                                stringsAsFactors = FALSE)
get_datatable(r_squared_changes) %>%
    formatRound(columns = c('original R squared', 'updated R squared'),
                digits = 3)
```

</div>

#### Scatter plots {.tab-display .unnumbered}

<div class="code-block">

```{r, better-scatter-plots, cache=TRUE}
for (variable_name in r_squared_changes[['original model']]) {
    bool_index <- r_squared_changes[['original model']] == variable_name
    r_squared <- round(r_squared_changes[bool_index, 'updated R squared'],
                       digits = 3)
    linear_model <- linear_models[[variable_name]]
    plot_simple_fit(variable_name, linear_model, r_squared)
}
```

</div>

### Encode nominal variables with numbers {.tabset}

#### Summary {.unnumbered}

Apply $k$-fold target encoding to each nominal variable.

#### Discussion {.unnumbered}

Target encoding encodes each category by the mean of the target values for
that category.

This method is prone to overfitting, but variants have been developed to
minimize overfitting:

  - $K$-fold target encoding randomly separates the training data into $k$ folds.
    Samples in a given fold are encoded using data not in that fold.
  - For categories with a small number of samples, the encoding is a mix of
    the category target mean and the global target mean.

Five folds were used here, and the weighting of the global target mean was
given by a sigmoid function that increased from $\approx 0$ to $\approx 1$ as
the number of samples in the category increased from 0 to 10.

For each sample in the test data, the encoding used for a given category was
randomly selected from the five encodings obtained from the training data.

Missing values were encoded using the global target mean.


#### Visualize encodings {.tab-display .unnumbered}

<div class="code-block">

```{r target-encode, cache=FALSE, echo=FALSE}
# Encode all categorical predictors.

# Convert MSSubClass to a character vector to simplify the encoding.  Note
# that 'MSSubClass' is a factor vector, while the rest of the categorical
# predictors are character vectors.  The reason that MSSubClass is handled
# differently is that the variable is a scattered set of integers that
# represent classes.  If this variable is converted to a character early in
# the current script, the ordering of factors for plots is lexical, which
# '190' coming before '20', which makes the plotted distribution of MSSubClass
# harder to understand.  Converting it from an integer vector (imported by
# read.csv) directly to a factor automatically gives factors ordered by
# integer value in the distribution plot.
train_data <- mutate(train_data, MSSubClass = as.character(MSSubClass))

for (variable_name in categorical_predictors) {
    # The encode function is defined in the script encode.R.
    train_data <- encode(train_data, variable_name)
}

```

```{r show-encoded, ref.label=c('target-encode', 'check-encoding'), eval=FALSE}
```

```{r check-encoding, cache=TRUE, echo=FALSE}
for (variable_name in large_eta_squared$name) {
    eta_squared <- round(eta_squared_vec[variable_name], digits = 3)
    generate_box_plot(variable_name, eta_squared, show_encoding = TRUE)
}
```

</div>

### Outcome of feature engineering {.tabset}

#### Summary {.unnumbered}

List the changes applied to variables in the original data set.

#### Details {.unnumbered}

Changes to the variables:

  1. The target (SalePrice) and one predictor (GrLivArea: Above ground living
     area) were both transformed using the logarithm.
  2. The predictors YearBuilt and GarageYrBlt were transformed using the
     function $f(x) = (x - 1900)^3$.
  3. Dummy variables NoBasement, NoGarage, NoFireplace, NoSecondFloor were
     defined in order to eliminate distortions in the fit for related numeric
     variables.
  4. Outliers for individual numeric predictors were defined.
  5. Each nominal variable in the training set was encoded using $k$-fold
     target encoding with $k=5$.  For categories with $<10$ samples, a sigmoid
     function mixed the global target mean with the category target mean to
     give the encoding.  As a natural extension of this step, missing
     categorical data was encoded with the global target mean.

### Additional possibilities for feature engineering {.tabset}

#### Summary {.unnumbered}

Discuss additional feature engineering that could lead to an improved linear
model.

#### Discussion {.unnumbered}

New variables could be defined by eliminating poorly sampled discrete values.

  - Example:  There are only a few houses in the training data with GarageCars
    equal to 4, and these houses all have a price lower than that predicted by
    the linear fit.  The model would likely be improved by merging the values
    3 and 4 in GarageCares, i.e., using the value 3 to represent all houses
    with 3 or more cars.

Methods of encoding categorical variables could be more fully explored using
cross validation for a particular model.

  - Example:  For k-fold target encoding, cross-validation could be used to
    tune the number of folds as well as the weight given to the global target
    mean for categories with a small number of samples.

## Are the predictors independent? {.tabset}

### Summary {.unnumbered}

Display a table of correlations between predictors as an aid to tuning the
multilinear model.

### Discussion {.unnumbered}

A correlation matrix was calculated that included the following 45 predictors:

  - Numeric predictors with large $R^2$.
  - Encoded versions of categorical predictors with large $\eta^2$.

In the table, correlations are presented in order of decreasing magnitude.

Many of the predictors that have a strong linear association with the target
are also strongly correlated with other predictors.

### Correlations {.tab-display .unnumbered}

<div class="code-block">

```{r, correlations, cache=FALSE}
linear_predictors <- c(large_r_squared$name,
                       paste0(large_eta_squared$name, 'Num'))

corr_matrix <- select(train_data, all_of(linear_predictors)) %>%
    cor(use = 'pairwise.complete.obs')
# Set to NA the upper triangular part of the correlation matrix.
bool_index <- lower.tri(corr_matrix, diag = TRUE)
corr_matrix[bool_index] <- NA

correlations <- as.data.frame(corr_matrix) %>%
    mutate(var_1 = rownames(.)) %>%
    pivot_longer(-var_1, names_to = 'var_2', values_to = 'corr') %>%
    filter(!is.na(corr)) %>%
    arrange(desc(abs(corr))) %>%
    rename(`predictor 1` = var_1, `predictor 2` = var_2,
           correlation = corr)
get_datatable(correlations) %>%
    formatRound(columns = 'correlation', digits = 3)
```

 </div>

# Multilinear model
